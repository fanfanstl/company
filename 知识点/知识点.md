

# 知识点

## 公司代理说明

![1558060674343](C:\Users\fanding\AppData\Local\Temp\1558060674343.png)

```
bbt开头的是国外代理，自己搭建的代理
qingting和xundaili是国内三方代理，生命周期不长
代理数据库：bbt_spider中spider_proxy_list表中
```

## 讯代理同步线下php脚本

xundaili.php

## 设置断点

```
设置断点：
	import pdb
	pdb.set_trace()
	c继续执行
```

```
jkids_hara 第二天任务
启动命令：scrapy crawl denaec  -a taskId=1231 -a taskType='spider_update' -a sourceUrls='["https://wowma.jp/item/307857399?l=true%26e%3DllA%26e2%3Dlisting_flpro"]'
```

## 抓取时单测详情如上

## git命令

```
git diff [filename] 比对文件前后差异
git status 查看当前状态
git commit -m "describe " [filename] 将单个文件提交到本地仓库
git push origin master 推到远程仓库上
```

## 价格断言保护

```
assert sourceOriginalPrice >= sourceSellingPrice, "sourceOriginalPrice[%s] is small than sourceSellingPrice[%s]" % (
        sourceOriginalPrice, sourceSellingPrice)
```

## scrapy中添加headers

![1551681477292](D:\360MoveData\Users\fanding\Desktop\company\知识点\1551681477292.png)

## scrapy中添加cookie

可以同上，在headers中添加cookie

![1551681743109](D:\360MoveData\Users\fanding\Desktop\company\知识点\1551681743109.png)

## git解决冲突

```
git checkout [文件名] ：从远程仓库同步代码
git pull :同步下来
然后git push origin master
```

## phantomjs使用

```

```

## selenium使用

```

```

## linux14.04安装docker

```
1、使用 sudo or root 权限登陆计算机.

2、打开 terminal window（命令窗口）.

3、更新安装包信息, 确保 APT 使用 https 协议, 同时CA 证书已经被安装.

 $ sudo apt-get update
 $ sudo apt-get install apt-transport-https ca-certificates
4、添加新的GPGkey.

$ sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D
5、用编辑器打开  /etc/apt/sources.list.d/docker.list.
如果不存在，则新建一个
删除任何现有输入.

6、添加与您Ubuntu操作系统相关条目。

该条目可以是：

On Ubuntu Precise 12.04 (LTS)

deb https://apt.dockerproject.org/repo ubuntu-precise main
On Ubuntu Trusty 14.04 (LTS)

deb https://apt.dockerproject.org/repo ubuntu-trusty main
Ubuntu Wily 15.10

deb https://apt.dockerproject.org/repo ubuntu-wily main
Ubuntu Xenial 16.04 (LTS)

deb https://apt.dockerproject.org/repo ubuntu-xenial main

7、更新APT 软件包索引.

$ sudo apt-get update
9、清除旧的repo if it exists.

$ sudo apt-get purge lxc-docker
10、确保 APT 是从正确的代码库拉取下来的.

$ apt-cache policy docker-engine

11、Update your APT package index.

$ sudo apt-get update
12、安装 Docker.

$ sudo apt-get install docker-engine
13、开始使用docker.

$ sudo service docker start
14、确认docker已被正确安装.

$ sudo docker run hello-world
```

## 环境搭建

![1552228408765](D:\360MoveData\Users\fanding\Desktop\company\知识点\1552228408765.png)

## 反爬虫

1、除了xhr请求还有可能是在js中，还有就是可能是js构造出来的数据

2、不同的代理请求出来的页面可能不一样









## 数据库配置文件

newWebsite.conf中添加spider的名字



## 单独运行某个详情页

scrapy crawl denaec  -a taskId=1231 -a taskType='spider_update' -a sourceUrls='["https://wowma.jp/item/307857399?l=true%26e%3DllA%26e2%3Dlisting_flpro"]'



## 脚本上线流程

后台进入操作：

品牌管理--->供应商品牌审核---->新增供应商品牌

配置：

供应商：spider_name

转运国家：品牌的发货地

源品牌：脚本里配置的，也可能是动态抓取出来的

棒棒糖品牌：邮件里写明的，注意确认、确认

品牌国家：邮件里有写明



运行spider

单品管理--->抓取任务列表----->添加任务

解释：

​	任务类型：spider：抓取全网    spider_update：抓取配置好的部分东西

​	网站：spider_name

​	更新库存：更新价格和库存

​	导入新品：导入新的商品

​	更新图片：更新图片



上线跑完以后检测流程：

​	单品管理--->上新列表---->核对信息

## FormRequest中的from_response方法

![1553074137525](D:\360MoveData\Users\fanding\Desktop\company\知识点\1553074137525.png)

## 抓取报警处理流程

1、进入线上抓取库

2、到后台任务列表中找到报警spider的taskId

2、查看错误日志

select * from spider_log where taskId=taskId

根据错误日志进行排错

## 采购报警处理

1、进入后台--->采购任务列表，搜索有问题的spidername

2、查看详情，找到在哪台主机上运行，并且找出任务id

3、进入该主机

4、进入到/home/work 目录下

5、grep 856038 spider-online-*/log/spider.log.2019032115->到小时

6、grep janieandjack spider-online-9/log/spider.log.2019032115

7、可以将出错的截屏用scp拉取下来

​	找到在哪台机器上运行的（ip）、看抛出异常的部分是在哪个实例下（找到spider-online-*）然后该目录下面有log目录，然后进入对应spidername下的目录，然后找到匹配的taskId进入即可看

```
scp命令
	scp from  地方  to 地方
	scp rd@127.0.0.1:/path/pp ./
	
cp命令
	cp同上
	cp from 地方 to 地方

```



```
扩展：
	grep命令：grep patten fileName
	
	规则表达式：
		grep的规则表达式:
            ^  #锚定行的开始 如：'^grep'匹配所有以grep开头的行。    

            $  #锚定行的结束 如：'grep$'匹配所有以grep结尾的行。    

            .  #匹配一个非换行符的字符 如：'gr.p'匹配gr后接一个任意字符，然后是p。    

            *  #匹配零个或多个先前字符 如：'*grep'匹配所有一个或多个空格后紧跟grep的行。    

            .*   #一起用代表任意字符。   

            []   #匹配一个指定范围内的字符，如'[Gg]rep'匹配Grep和grep。    

            [^]  #匹配一个不在指定范围内的字符，如：'[^A-FH-Z]rep'匹配不包含A-R和T-Z的一个字母开头，紧跟rep的行。    

            \(..\)  #标记匹配字符，如'\(love\)'，love被标记为1。    

            \<      #锚定单词的开始，如:'\<grep'匹配包含以grep开头的单词的行。    

            \>      #锚定单词的结束，如'grep\>'匹配包含以grep结尾的单词的行。    

            x\{m\}  #重复字符x，m次，如：'0\{5\}'匹配包含5个o的行。    

            x\{m,\}  #重复字符x,至少m次，如：'o\{5,\}'匹配至少有5个o的行。    

            x\{m,n\}  #重复字符x，至少m次，不多于n次，如：'o\{5,10\}'匹配5--10个o的行。   

            \w    #匹配文字和数字字符，也就是[A-Za-z0-9]，如：'G\w*p'匹配以G后跟零个或多个文字或数字字符，然后是p。   

            \W    #\w的反置形式，匹配一个或多个非单词字符，如点号句号等。   

            \b    #单词锁定符，如: '\bgrep\b'只匹配grep。  
		
```

## excel操作

cat data | awk '{print "\""$1"\","}' 连接的时候好比+号，只是什么都不适用即可连接

## 线上任务同步到线下

```
bash syncCeleryTaskById.sh 876530

1、找到任务id
1、对应的bash脚本是syncCeleryTaskById.sh
```

## 抓取价格时需要添加断言判断

```
assert sourceOriginalPrice >= sourceSellingPrice, "sourceOriginalPrice[%s] is small than sourceSellingPrice[%s]" % (sourceOriginalPrice,sourceSellingPrice)
```

## spiderdb中bbt_spider中的spider_log表和线上每个实例中的log日志

```
抓取业务：错误日志存储在spiderdb（线上库）中bbt_spider中的spider_log中
采购业务：错误日志存储在不同的实例中
```

## h2m使用

https://github.com/island205/h2m



## WordPress 地址和密码

http://52.80.35.131:8100/wp-login.php?redirect_to=http%3A%2F%2F52.80.35.131%3A8100%2Fwp-admin%2F&reauth=1

bangbangtang

bbt@20150920

# php相关知识点

## php错误日志存放

```
日志存放处：/home/work/env/log
如下图：
	php：存放总的错误日志
	db：存放数据库操作错误日志
	其他文件存放对应模块的错误日志
	注意：
		wf:为错误日志
```

![1555920340612](D:\360MoveData\Users\fanding\Desktop\company\知识点\1555920340612.png)



## 图片上传处理PHP接口uploadPic

![1555903175562](D:\360MoveData\Users\fanding\Desktop\company\知识点\1555903175562.png)

```
有关sql语句：
	图片下载下来异步上传到七牛云上将结果存储到bbt_spider中的spider_pic表中
    
    select * from spider_pic where picUrl='https://pic1.zhimg.com/v2-				d4650a3fceb8150821bf31628efb8f46_r.jpg'\G;
```



## addForumData.php定时任务：更新wordpress中的url为自己下上传到七牛云上的地址

```
// */15 * * * * cd /home/work/env/app/spider/scripts/crontab/ && /home/work/env/php/bin/php addForumData.php


```

手动添加任务过程



























## 自动添加spider任务和spider_update任务

```


gap例子：
https://bonbonadmin.bbtkids.cn/1.0/uiitem/viewwebsite?websiteid=102563

对应数据库配置：
    在testdb中的bbt库中
    select config from supplier_website where 		sourceWebsite = 'gap'\G;

对应php添加spider任务脚本：
spider/application/scripts/crontab/addNewTask.php

对应php添加spider_update任务脚本：
spider/application/scripts/crontab/addUpdateTask.php

```

## 任务信息相关存储库

```
spiderdb--->bbt_spider--->scrapy_task--->可通过sourceItemVersion进行单条查询
```

## 新增供应商品牌逻辑

```
uiitem/application/actions下的viewWebsiteBrand文件
```

# 添加任务

```
$ret = $client->call('task', 'getTask', $req); 查询任务
$ret = $client->call('task', 'addTask', $req); 添加spider_update任务
$ret = $client->call('task', 'addScrapyTask', $req); 添加spiderscrapy任务
$ret = $client->call('spider','addNewTask',$req);
```
# 线下生成query_order任务

1、通过采购单号到线上后台：查找采购单->合同号
2、根据合同号去线上spiderdb中查找select * from spider_express where sourceOrderId='合同号';
将用户名和密码配置到addShoppingTask PHP脚本中。

ctrl+f查找供应商名字---》修改对应用户名、密码、合同号

然后执行该脚本执行命令

phpe addShoppingTask.php chocoel（供应商名称） order

生成一个线下的query_order任务。

 

通过采购单号：
查询采购任务id流程

select * from celery_task where extra like %（合同号还是采购单号，忘记了，都试一下）%;
查询出taskId然后再去跑这个shopping任务。







# addNewTask.php脚本解读

```bash
// 1 */1 * * * cd /home/work/env/app/spider/scripts/crontab/ && /home/work/env/php/bin/php addNewTask.php all

```

# 讯代理三方链接

```
http://www.xdaili.cn/ipagent/privateProxy/applyStaticProxy?count=2&spiderId=548dc6ba482e48cc94f0d685a5017689&returnType=2
```
# 对添加spider任务和spider_update任务的理解

```
task表和scrapy_task表
task表放spider任务和spider_update的主任务，而scrapy_task放spider_update的分片任务
```

# 环境搭建初始化配置

注释掉setting.py文件的

```
LOG_FILE = "log/spider.log"
```

conf目录下创建env.py文件

```
#!/usr/bin/python
# -*- coding: utf-8 -*-
env_type = "offline"
```

在spider-python目录下创建log目录

# 重试优化

装饰器实现





# 社区后台逻辑

发布前的摘要配置：http://bbtwp.bangbangtown.cn:8100/wp-admin/post.php?post=10710&action=edit

手机抓取小红书配置教程：http://bbtwiki.bangbangtown.cn/doku.php?id=http:spider&s[]=proxy

海哥代理数据拦截：http://bbtgit.bangbangtown.cn/bbt/bbt_node_spider

# 任务调度

查找配置文件，将所有供应商和config做成字典

$spiderPool布尔值开关

开：

​	判断该机器是否是执行淘宝的机器

​		是：

​			遍历上面的字典，如果是淘宝供应商就进入构建$_newTaskConfig字典（供应商：config）

​		否：

​			遍历上面的字典，找出非淘宝供应商，构架$_newTaskConfig字典（供应商：config)

获取special_spider类型任务



获取spider_update任务

获取spider任务